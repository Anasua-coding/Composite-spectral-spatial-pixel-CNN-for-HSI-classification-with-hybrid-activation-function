{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "99GIVgTAYQ6y"
      },
      "source": [
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKtr6mN6Ynrn"
      },
      "source": [
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h8FkWsgiZy6k"
      },
      "source": [
        "\n",
        "downloaded = drive.CreateFile({'id':'1-Pi2LI3msqoA1FliwDTi_8DaaHwimqQC'}) # replace the id with id of file you want to access\n",
        "downloaded.GetContentFile('KSC.mat')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IpE_ZE94boT9"
      },
      "source": [
        "downloaded1 = drive.CreateFile({'id':'1F8lSkh3zoh8oGFWmigqh62jf70xkFJCG'}) # replace the id with id of file you want to access\n",
        "downloaded1.GetContentFile('KSC_gt.mat')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcd-h_BL8U55"
      },
      "source": [
        "%load_ext tensorboard\n",
        "import datetime, os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hZROIeKZRMly",
        "outputId": "d229268b-ef13-443d-9a2f-89bc29354cba"
      },
      "source": [
        "!pip install -q keras\n",
        "!pip install h5py ppyaml\n",
        "!pip install spectral\n",
        "import spectral"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (2.10.0)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement ppyaml (from versions: none)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for ppyaml\u001b[0m\n",
            "Collecting spectral\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ce/06/6a89035cde4eac3ed94e1888f850af653386e8ee827edc72ffc8e445bcb7/spectral-0.22.1-py3-none-any.whl (212kB)\n",
            "\u001b[K     |████████████████████████████████| 215kB 5.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from spectral) (1.19.5)\n",
            "Installing collected packages: spectral\n",
            "Successfully installed spectral-0.22.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ICxsI_xcb_V0",
        "outputId": "f9c22f9c-3637-43b5-c659-89e9f303fa2e"
      },
      "source": [
        "\n",
        "from scipy.io import loadmat\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, cohen_kappa_score\n",
        "from operator import truediv\n",
        "import seaborn as sn\n",
        "import keras\n",
        "from sklearn.utils import class_weight\n",
        "from keras import backend as A\n",
        "from keras.backend import sigmoid\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.models import Sequential\n",
        "from sklearn.utils import shuffle\n",
        "from tensorflow.keras.layers import BatchNormalization,Dense, Dropout, Activation, Flatten, Conv2D , Conv3D,MaxPooling2D, Reshape,Input,MaxPooling3D,Conv1D\n",
        "from keras.optimizers import Adam, SGD, RMSprop\n",
        "from keras.utils.generic_utils import get_custom_objects\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "m = loadmat('KSC.mat')\n",
        "gt= loadmat('KSC_gt.mat')\n",
        "print(m.keys())\n",
        "print(gt.keys())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.4.0\n",
            "dict_keys(['__header__', '__version__', '__globals__', 'KSC'])\n",
            "dict_keys(['__header__', '__version__', '__globals__', 'KSC_gt'])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q1Zl85qgcnlY",
        "outputId": "da1f2b28-3af2-4208-f701-afeaabacdf81"
      },
      "source": [
        "m['KSC'].shape\n",
        "gt['KSC_gt'].shape\n",
        "a=m['KSC']\n",
        "print(a.shape)\n",
        "groundtruth=gt['KSC_gt']\n",
        "print(groundtruth.shape)\n",
        "d=[]\n",
        "temp=[]\n",
        "gth=[]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(512, 614, 176)\n",
            "(512, 614)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bM0O-Vw3cvm7"
      },
      "source": [
        "for j in range(512):                                 # 3d image to 2d image columns= Bands rows = pixel\n",
        "    for k in range(614):\n",
        "        temp=a[j,k,:]\n",
        "        d.append(temp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n0zwloj1c60P"
      },
      "source": [
        "for l in range(512):                                # 2d image of ground truth to 1 d image with single column (target)\n",
        "    for s in range(614):\n",
        "        gtemp=groundtruth[l,s]\n",
        "        gth.append(gtemp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "id": "1d2CPwZjdCdh",
        "outputId": "4285285d-8f64-44b1-8828-6f46d14a57c4"
      },
      "source": [
        "Img_target=pd.DataFrame(gth,columns=['target'])\n",
        "targetclasssamples=Img_target['target'].value_counts()\n",
        "tq=pd.DataFrame(targetclasssamples)\n",
        "tq"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>309157</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>927</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>761</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>520</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>503</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>431</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>419</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>404</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>252</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>243</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>229</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>161</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>105</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    target\n",
              "0   309157\n",
              "13     927\n",
              "1      761\n",
              "9      520\n",
              "12     503\n",
              "8      431\n",
              "11     419\n",
              "10     404\n",
              "3      256\n",
              "4      252\n",
              "2      243\n",
              "6      229\n",
              "5      161\n",
              "7      105"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MAAHoSkMdYK8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "7e0dc582-061e-4779-db38-8657bb4559fa"
      },
      "source": [
        "Img_features=pd.DataFrame(d)\n",
        "from sklearn.preprocessing import StandardScaler     # code for PCA while using scaling\n",
        "#from sklearn.preprocessing import MinMaxScaler\n",
        "scaler=StandardScaler()\n",
        "#scaler=MinMaxScaler((-0.5,0.5))\n",
        "scaler.fit(Img_features)\n",
        "scaled_data=scaler.transform(Img_features)\n",
        "Img_feature_scaled=pd.DataFrame(scaled_data)\n",
        "print(scaled_data.shape)\n",
        "Img_feature_scaled.head(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(314368, 176)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>136</th>\n",
              "      <th>137</th>\n",
              "      <th>138</th>\n",
              "      <th>139</th>\n",
              "      <th>140</th>\n",
              "      <th>141</th>\n",
              "      <th>142</th>\n",
              "      <th>143</th>\n",
              "      <th>144</th>\n",
              "      <th>145</th>\n",
              "      <th>146</th>\n",
              "      <th>147</th>\n",
              "      <th>148</th>\n",
              "      <th>149</th>\n",
              "      <th>150</th>\n",
              "      <th>151</th>\n",
              "      <th>152</th>\n",
              "      <th>153</th>\n",
              "      <th>154</th>\n",
              "      <th>155</th>\n",
              "      <th>156</th>\n",
              "      <th>157</th>\n",
              "      <th>158</th>\n",
              "      <th>159</th>\n",
              "      <th>160</th>\n",
              "      <th>161</th>\n",
              "      <th>162</th>\n",
              "      <th>163</th>\n",
              "      <th>164</th>\n",
              "      <th>165</th>\n",
              "      <th>166</th>\n",
              "      <th>167</th>\n",
              "      <th>168</th>\n",
              "      <th>169</th>\n",
              "      <th>170</th>\n",
              "      <th>171</th>\n",
              "      <th>172</th>\n",
              "      <th>173</th>\n",
              "      <th>174</th>\n",
              "      <th>175</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.150595</td>\n",
              "      <td>-0.512081</td>\n",
              "      <td>-0.078150</td>\n",
              "      <td>-0.028519</td>\n",
              "      <td>-0.597387</td>\n",
              "      <td>-0.421616</td>\n",
              "      <td>-0.385050</td>\n",
              "      <td>-0.278524</td>\n",
              "      <td>-0.190040</td>\n",
              "      <td>-0.145990</td>\n",
              "      <td>-0.120404</td>\n",
              "      <td>-0.061566</td>\n",
              "      <td>0.040374</td>\n",
              "      <td>0.115593</td>\n",
              "      <td>0.096897</td>\n",
              "      <td>0.144657</td>\n",
              "      <td>0.070483</td>\n",
              "      <td>0.019635</td>\n",
              "      <td>0.043065</td>\n",
              "      <td>0.056913</td>\n",
              "      <td>0.118609</td>\n",
              "      <td>0.072602</td>\n",
              "      <td>0.138181</td>\n",
              "      <td>0.159558</td>\n",
              "      <td>0.119938</td>\n",
              "      <td>0.144715</td>\n",
              "      <td>0.139276</td>\n",
              "      <td>0.179864</td>\n",
              "      <td>0.199204</td>\n",
              "      <td>0.222347</td>\n",
              "      <td>0.285957</td>\n",
              "      <td>0.345325</td>\n",
              "      <td>0.578530</td>\n",
              "      <td>0.735998</td>\n",
              "      <td>0.844114</td>\n",
              "      <td>0.890127</td>\n",
              "      <td>0.942512</td>\n",
              "      <td>0.920828</td>\n",
              "      <td>0.922117</td>\n",
              "      <td>0.929782</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.064352</td>\n",
              "      <td>-0.045921</td>\n",
              "      <td>-0.060034</td>\n",
              "      <td>-0.067739</td>\n",
              "      <td>-0.057954</td>\n",
              "      <td>-0.050325</td>\n",
              "      <td>-0.050346</td>\n",
              "      <td>-0.049237</td>\n",
              "      <td>-0.043080</td>\n",
              "      <td>-0.033127</td>\n",
              "      <td>-0.042435</td>\n",
              "      <td>-0.035593</td>\n",
              "      <td>-0.045520</td>\n",
              "      <td>-0.034335</td>\n",
              "      <td>-0.039067</td>\n",
              "      <td>-0.020848</td>\n",
              "      <td>-0.038101</td>\n",
              "      <td>-0.026404</td>\n",
              "      <td>-0.022064</td>\n",
              "      <td>-0.048072</td>\n",
              "      <td>-0.049034</td>\n",
              "      <td>-0.059682</td>\n",
              "      <td>-0.061505</td>\n",
              "      <td>-0.081958</td>\n",
              "      <td>-0.074012</td>\n",
              "      <td>-0.078594</td>\n",
              "      <td>-0.070513</td>\n",
              "      <td>-0.080835</td>\n",
              "      <td>-0.070239</td>\n",
              "      <td>-0.066579</td>\n",
              "      <td>-0.070182</td>\n",
              "      <td>-0.094691</td>\n",
              "      <td>-0.072993</td>\n",
              "      <td>-0.095777</td>\n",
              "      <td>-0.104324</td>\n",
              "      <td>-0.082610</td>\n",
              "      <td>-0.089427</td>\n",
              "      <td>-0.152843</td>\n",
              "      <td>-0.112849</td>\n",
              "      <td>-0.145075</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.151323</td>\n",
              "      <td>-0.745358</td>\n",
              "      <td>-0.154348</td>\n",
              "      <td>-0.046180</td>\n",
              "      <td>-1.074386</td>\n",
              "      <td>-0.978712</td>\n",
              "      <td>-0.775030</td>\n",
              "      <td>-0.691885</td>\n",
              "      <td>-0.674380</td>\n",
              "      <td>-0.565672</td>\n",
              "      <td>-0.521417</td>\n",
              "      <td>-0.500192</td>\n",
              "      <td>-0.457551</td>\n",
              "      <td>-0.364119</td>\n",
              "      <td>-0.301715</td>\n",
              "      <td>-0.351637</td>\n",
              "      <td>-0.389254</td>\n",
              "      <td>-0.438962</td>\n",
              "      <td>-0.433632</td>\n",
              "      <td>-0.479218</td>\n",
              "      <td>-0.428080</td>\n",
              "      <td>-0.440177</td>\n",
              "      <td>-0.431114</td>\n",
              "      <td>-0.399391</td>\n",
              "      <td>-0.389496</td>\n",
              "      <td>-0.391305</td>\n",
              "      <td>-0.411348</td>\n",
              "      <td>-0.397899</td>\n",
              "      <td>-0.387551</td>\n",
              "      <td>-0.417190</td>\n",
              "      <td>-0.345524</td>\n",
              "      <td>-0.246946</td>\n",
              "      <td>-0.017248</td>\n",
              "      <td>0.282042</td>\n",
              "      <td>0.493604</td>\n",
              "      <td>0.669474</td>\n",
              "      <td>0.720312</td>\n",
              "      <td>0.763848</td>\n",
              "      <td>0.744910</td>\n",
              "      <td>0.763712</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.070022</td>\n",
              "      <td>-0.055166</td>\n",
              "      <td>-0.066120</td>\n",
              "      <td>-0.074767</td>\n",
              "      <td>-0.065024</td>\n",
              "      <td>-0.060221</td>\n",
              "      <td>-0.059764</td>\n",
              "      <td>-0.058717</td>\n",
              "      <td>-0.053004</td>\n",
              "      <td>-0.044314</td>\n",
              "      <td>-0.052158</td>\n",
              "      <td>-0.045269</td>\n",
              "      <td>-0.055203</td>\n",
              "      <td>-0.046781</td>\n",
              "      <td>-0.049348</td>\n",
              "      <td>-0.032320</td>\n",
              "      <td>-0.048860</td>\n",
              "      <td>-0.040854</td>\n",
              "      <td>-0.035543</td>\n",
              "      <td>-0.056431</td>\n",
              "      <td>-0.056610</td>\n",
              "      <td>-0.067276</td>\n",
              "      <td>-0.067624</td>\n",
              "      <td>-0.087087</td>\n",
              "      <td>-0.078663</td>\n",
              "      <td>-0.082484</td>\n",
              "      <td>-0.076768</td>\n",
              "      <td>-0.086589</td>\n",
              "      <td>-0.075716</td>\n",
              "      <td>-0.073396</td>\n",
              "      <td>-0.075113</td>\n",
              "      <td>-0.099080</td>\n",
              "      <td>-0.078946</td>\n",
              "      <td>-0.098888</td>\n",
              "      <td>-0.108069</td>\n",
              "      <td>-0.087396</td>\n",
              "      <td>-0.092110</td>\n",
              "      <td>-0.155371</td>\n",
              "      <td>-0.115533</td>\n",
              "      <td>-0.146667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.151115</td>\n",
              "      <td>-1.095274</td>\n",
              "      <td>-0.162814</td>\n",
              "      <td>-0.046180</td>\n",
              "      <td>-1.021386</td>\n",
              "      <td>-0.928067</td>\n",
              "      <td>-0.921272</td>\n",
              "      <td>-0.783743</td>\n",
              "      <td>-0.718411</td>\n",
              "      <td>-0.691576</td>\n",
              "      <td>-0.601620</td>\n",
              "      <td>-0.579942</td>\n",
              "      <td>-0.534155</td>\n",
              "      <td>-0.474822</td>\n",
              "      <td>-0.482902</td>\n",
              "      <td>-0.493435</td>\n",
              "      <td>-0.495347</td>\n",
              "      <td>-0.544792</td>\n",
              "      <td>-0.569831</td>\n",
              "      <td>-0.546235</td>\n",
              "      <td>-0.492396</td>\n",
              "      <td>-0.472226</td>\n",
              "      <td>-0.525997</td>\n",
              "      <td>-0.492549</td>\n",
              "      <td>-0.479396</td>\n",
              "      <td>-0.480642</td>\n",
              "      <td>-0.440328</td>\n",
              "      <td>-0.455675</td>\n",
              "      <td>-0.534240</td>\n",
              "      <td>-0.533470</td>\n",
              "      <td>-0.517746</td>\n",
              "      <td>-0.472573</td>\n",
              "      <td>-0.215841</td>\n",
              "      <td>0.055064</td>\n",
              "      <td>0.283298</td>\n",
              "      <td>0.478241</td>\n",
              "      <td>0.576536</td>\n",
              "      <td>0.631019</td>\n",
              "      <td>0.567703</td>\n",
              "      <td>0.597643</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.068278</td>\n",
              "      <td>-0.052645</td>\n",
              "      <td>-0.064013</td>\n",
              "      <td>-0.073527</td>\n",
              "      <td>-0.063610</td>\n",
              "      <td>-0.057617</td>\n",
              "      <td>-0.056887</td>\n",
              "      <td>-0.057664</td>\n",
              "      <td>-0.050085</td>\n",
              "      <td>-0.041168</td>\n",
              "      <td>-0.049212</td>\n",
              "      <td>-0.042266</td>\n",
              "      <td>-0.051053</td>\n",
              "      <td>-0.043833</td>\n",
              "      <td>-0.046022</td>\n",
              "      <td>-0.028790</td>\n",
              "      <td>-0.044557</td>\n",
              "      <td>-0.036408</td>\n",
              "      <td>-0.031459</td>\n",
              "      <td>-0.052386</td>\n",
              "      <td>-0.052822</td>\n",
              "      <td>-0.063824</td>\n",
              "      <td>-0.066491</td>\n",
              "      <td>-0.085319</td>\n",
              "      <td>-0.077500</td>\n",
              "      <td>-0.081928</td>\n",
              "      <td>-0.075154</td>\n",
              "      <td>-0.084431</td>\n",
              "      <td>-0.074499</td>\n",
              "      <td>-0.069775</td>\n",
              "      <td>-0.072647</td>\n",
              "      <td>-0.097669</td>\n",
              "      <td>-0.073985</td>\n",
              "      <td>-0.098110</td>\n",
              "      <td>-0.105909</td>\n",
              "      <td>-0.084560</td>\n",
              "      <td>-0.090098</td>\n",
              "      <td>-0.154359</td>\n",
              "      <td>-0.115130</td>\n",
              "      <td>-0.146030</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.149763</td>\n",
              "      <td>-0.103845</td>\n",
              "      <td>-0.061217</td>\n",
              "      <td>-0.024594</td>\n",
              "      <td>-0.385388</td>\n",
              "      <td>-0.370971</td>\n",
              "      <td>-0.238808</td>\n",
              "      <td>-0.094808</td>\n",
              "      <td>-0.057947</td>\n",
              "      <td>0.063851</td>\n",
              "      <td>0.080103</td>\n",
              "      <td>0.058059</td>\n",
              "      <td>0.078676</td>\n",
              "      <td>0.152494</td>\n",
              "      <td>0.169371</td>\n",
              "      <td>0.215556</td>\n",
              "      <td>0.176576</td>\n",
              "      <td>0.160742</td>\n",
              "      <td>0.213314</td>\n",
              "      <td>0.224454</td>\n",
              "      <td>0.311558</td>\n",
              "      <td>0.361040</td>\n",
              "      <td>0.422829</td>\n",
              "      <td>0.407980</td>\n",
              "      <td>0.479537</td>\n",
              "      <td>0.502062</td>\n",
              "      <td>0.573979</td>\n",
              "      <td>0.584299</td>\n",
              "      <td>0.404568</td>\n",
              "      <td>0.425836</td>\n",
              "      <td>0.486882</td>\n",
              "      <td>0.514546</td>\n",
              "      <td>0.504058</td>\n",
              "      <td>0.509020</td>\n",
              "      <td>0.546180</td>\n",
              "      <td>0.537082</td>\n",
              "      <td>0.498113</td>\n",
              "      <td>0.510265</td>\n",
              "      <td>0.529730</td>\n",
              "      <td>0.502746</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.052792</td>\n",
              "      <td>-0.032195</td>\n",
              "      <td>-0.046692</td>\n",
              "      <td>-0.057405</td>\n",
              "      <td>-0.046171</td>\n",
              "      <td>-0.038346</td>\n",
              "      <td>-0.037527</td>\n",
              "      <td>-0.038177</td>\n",
              "      <td>-0.028777</td>\n",
              "      <td>-0.013899</td>\n",
              "      <td>-0.026231</td>\n",
              "      <td>-0.015907</td>\n",
              "      <td>-0.031134</td>\n",
              "      <td>-0.015993</td>\n",
              "      <td>-0.020621</td>\n",
              "      <td>0.007389</td>\n",
              "      <td>-0.019658</td>\n",
              "      <td>-0.004544</td>\n",
              "      <td>-0.000415</td>\n",
              "      <td>-0.031086</td>\n",
              "      <td>-0.032801</td>\n",
              "      <td>-0.047946</td>\n",
              "      <td>-0.050626</td>\n",
              "      <td>-0.073469</td>\n",
              "      <td>-0.065096</td>\n",
              "      <td>-0.071742</td>\n",
              "      <td>-0.061432</td>\n",
              "      <td>-0.073283</td>\n",
              "      <td>-0.062328</td>\n",
              "      <td>-0.057207</td>\n",
              "      <td>-0.061552</td>\n",
              "      <td>-0.088265</td>\n",
              "      <td>-0.064857</td>\n",
              "      <td>-0.088312</td>\n",
              "      <td>-0.097555</td>\n",
              "      <td>-0.075343</td>\n",
              "      <td>-0.080538</td>\n",
              "      <td>-0.148799</td>\n",
              "      <td>-0.106273</td>\n",
              "      <td>-0.141784</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.149971</td>\n",
              "      <td>0.246071</td>\n",
              "      <td>0.023447</td>\n",
              "      <td>-0.004969</td>\n",
              "      <td>0.197610</td>\n",
              "      <td>0.287415</td>\n",
              "      <td>0.297414</td>\n",
              "      <td>0.456340</td>\n",
              "      <td>0.514456</td>\n",
              "      <td>0.525501</td>\n",
              "      <td>0.681623</td>\n",
              "      <td>0.775809</td>\n",
              "      <td>0.844715</td>\n",
              "      <td>0.890512</td>\n",
              "      <td>0.930357</td>\n",
              "      <td>0.924548</td>\n",
              "      <td>0.919227</td>\n",
              "      <td>0.901552</td>\n",
              "      <td>0.962409</td>\n",
              "      <td>0.995144</td>\n",
              "      <td>1.051196</td>\n",
              "      <td>1.066112</td>\n",
              "      <td>1.118634</td>\n",
              "      <td>1.122192</td>\n",
              "      <td>1.198737</td>\n",
              "      <td>1.216756</td>\n",
              "      <td>1.240524</td>\n",
              "      <td>1.277615</td>\n",
              "      <td>1.314039</td>\n",
              "      <td>1.297931</td>\n",
              "      <td>1.319288</td>\n",
              "      <td>1.473461</td>\n",
              "      <td>1.521845</td>\n",
              "      <td>1.520104</td>\n",
              "      <td>1.439981</td>\n",
              "      <td>1.346145</td>\n",
              "      <td>1.295417</td>\n",
              "      <td>1.258939</td>\n",
              "      <td>1.263875</td>\n",
              "      <td>1.250058</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.041233</td>\n",
              "      <td>-0.019028</td>\n",
              "      <td>-0.034520</td>\n",
              "      <td>-0.046450</td>\n",
              "      <td>-0.032503</td>\n",
              "      <td>-0.022721</td>\n",
              "      <td>-0.021830</td>\n",
              "      <td>-0.021850</td>\n",
              "      <td>-0.010096</td>\n",
              "      <td>0.007777</td>\n",
              "      <td>-0.007374</td>\n",
              "      <td>0.003445</td>\n",
              "      <td>-0.012598</td>\n",
              "      <td>0.004642</td>\n",
              "      <td>-0.002477</td>\n",
              "      <td>0.036068</td>\n",
              "      <td>0.000323</td>\n",
              "      <td>0.019910</td>\n",
              "      <td>0.029812</td>\n",
              "      <td>-0.015987</td>\n",
              "      <td>-0.017651</td>\n",
              "      <td>-0.035521</td>\n",
              "      <td>-0.039294</td>\n",
              "      <td>-0.064980</td>\n",
              "      <td>-0.054824</td>\n",
              "      <td>-0.061370</td>\n",
              "      <td>-0.052352</td>\n",
              "      <td>-0.065731</td>\n",
              "      <td>-0.052592</td>\n",
              "      <td>-0.048899</td>\n",
              "      <td>-0.054977</td>\n",
              "      <td>-0.080585</td>\n",
              "      <td>-0.054737</td>\n",
              "      <td>-0.081003</td>\n",
              "      <td>-0.091939</td>\n",
              "      <td>-0.067721</td>\n",
              "      <td>-0.074165</td>\n",
              "      <td>-0.144350</td>\n",
              "      <td>-0.102649</td>\n",
              "      <td>-0.138174</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 176 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        0         1         2    ...       173       174       175\n",
              "0 -0.150595 -0.512081 -0.078150  ... -0.152843 -0.112849 -0.145075\n",
              "1 -0.151323 -0.745358 -0.154348  ... -0.155371 -0.115533 -0.146667\n",
              "2 -0.151115 -1.095274 -0.162814  ... -0.154359 -0.115130 -0.146030\n",
              "3 -0.149763 -0.103845 -0.061217  ... -0.148799 -0.106273 -0.141784\n",
              "4 -0.149971  0.246071  0.023447  ... -0.144350 -0.102649 -0.138174\n",
              "\n",
              "[5 rows x 176 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kOztbUrydoP_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "459c183b-9593-49b1-ab60-53a4eebfe98b"
      },
      "source": [
        "from sklearn.decomposition import PCA              # Applying PCA and obtainng variance in different PC bands.\n",
        "bands=15\n",
        "pca=PCA(n_components=bands)\n",
        "pca.fit(scaled_data)     # scaled Image\n",
        "#pca.fit(Img_features)     #original without scaling\n",
        "pca_image=pca.transform(scaled_data)\n",
        "#pca_image=pca.transform(Img_features)\n",
        "variance=pca.explained_variance_ratio_\n",
        "print(variance)\n",
        "print(\"___________________________________________\")\n",
        "print(pca.singular_values_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.2870401  0.11563006 0.10909688 0.04017486 0.02489982 0.01218961\n",
            " 0.00936347 0.00910299 0.00822749 0.0068597  0.00650285 0.00595623\n",
            " 0.00575095 0.00574214 0.00554059]\n",
            "___________________________________________\n",
            "[3985.16940157 2529.36131807 2456.86713901 1490.91425485 1173.74461099\n",
            "  821.24044408  719.77026612  709.68807156  674.69742661  616.06726462\n",
            "  599.82904289  574.06542632  564.08607333  563.65371596  553.67325228]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cKBKrhIjdyo8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f69e6d20-e883-4788-9ca1-58614abb7f43"
      },
      "source": [
        "Frame_pca_image=pd.DataFrame(pca_image)\n",
        "Frame_pca_image.head(5)\n",
        "print(Frame_pca_image.shape)\n",
        "PCAband=[]                   #converting 2D PCA matrix to 3D with bands=components\n",
        "#PCAbandFinal=[]\n",
        "bandtemp=[]\n",
        "#for m in range(pca_image.shape[1]):\n",
        "for s in range(512):\n",
        "        bandtemp=pca_image[s*614:(s+1)*614]\n",
        "        PCAband.append(bandtemp)\n",
        "    #PCAbandFinal.append(PCAband)\n",
        "PCAbandFinalarray=np.asarray(PCAband)   # converting the 3D list as array.\n",
        "print(PCAbandFinalarray.shape)\n",
        "PCAbandFinalarray[1,1,1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(314368, 15)\n",
            "(512, 614, 15)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3.2443432625594837"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VhgFxxMad7DN"
      },
      "source": [
        "def padWithZeros(X, margin=2):    # Padding the image with zeroes and is dedendent on the window or patch size\n",
        "    newX = np.zeros((X.shape[0] + 2 * margin, X.shape[1] + 2* margin, X.shape[2]))\n",
        "    x_offset = margin\n",
        "    y_offset = margin\n",
        "    newX[x_offset:X.shape[0] + x_offset, y_offset:X.shape[1] + y_offset, :] = X\n",
        "    return newX\n",
        "\n",
        "def createImageCubes(X, y, windowSize=5, removeZeroLabels = True):  #creating image patches with window size.\n",
        "    margin = int((windowSize - 1) / 2)\n",
        "    print(margin)\n",
        "    zeroPaddedX = padWithZeros(X, margin=margin)\n",
        "    # split patches\n",
        "    print(X.shape[0] * X.shape[1])\n",
        "    patchesData = np.zeros((X.shape[0] * X.shape[1], windowSize, windowSize, X.shape[2]))\n",
        "    patchesLabels = np.zeros((X.shape[0] * X.shape[1]))\n",
        "    patchIndex = 0\n",
        "    for r in range(margin, zeroPaddedX.shape[0] - margin):\n",
        "        for c in range(margin, zeroPaddedX.shape[1] - margin):\n",
        "            patch = zeroPaddedX[r - margin:r + margin + 1, c - margin:c + margin + 1]\n",
        "            patchesData[patchIndex, :, :, :] = patch\n",
        "            patchesLabels[patchIndex] = y[r-margin, c-margin]\n",
        "            patchIndex = patchIndex + 1\n",
        "    if removeZeroLabels:\n",
        "        patchesData = patchesData[patchesLabels>0,:,:,:]\n",
        "        patchesLabels = patchesLabels[patchesLabels>0]\n",
        "        patchesLabels -= 1\n",
        "    return patchesData, patchesLabels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vQgeH2iteJD9",
        "outputId": "82c6a72c-73c6-4bc6-f4d0-07c140a05944"
      },
      "source": [
        "Feature_Image, Target = createImageCubes(PCAbandFinalarray, groundtruth, windowSize=25)\n",
        "print(Feature_Image.shape)\n",
        "print(Target.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "12\n",
            "314368\n",
            "(5211, 25, 25, 15)\n",
            "(5211,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCiJ9muzebVS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "814f326f-f91b-406a-a3c7-26d8891bac92"
      },
      "source": [
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "trainsize=10\n",
        "testsize= ((100-trainsize)/100)\n",
        "print(testsize)\n",
        "Feature_Image1,Target1 = shuffle(Feature_Image,Target)\n",
        "x_train, x_test, y_train, y_test = train_test_split(Feature_Image1,Target1,test_size=testsize,stratify=Target1)\n",
        "#x_train, x_test, y_train, y_test = train_test_split(Feature_Image,Target,test_size=testsize,stratify=Target)\n",
        "YtrainFrame=pd.DataFrame(y_train,columns=['target'])\n",
        "y=YtrainFrame['target']\n",
        "z=y.value_counts(ascending=True)\n",
        "yq=pd.DataFrame(z)\n",
        "print(\"Final Training Samples \", yq )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9\n",
            "Final Training Samples        target\n",
            "6.0       11\n",
            "4.0       16\n",
            "5.0       23\n",
            "1.0       24\n",
            "3.0       25\n",
            "2.0       26\n",
            "9.0       40\n",
            "10.0      42\n",
            "7.0       43\n",
            "11.0      50\n",
            "8.0       52\n",
            "0.0       76\n",
            "12.0      93\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-sJmILTfW7f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1eb389d4-1c31-4af3-e8ac-8318e56fc54a"
      },
      "source": [
        "windowSize=25\n",
        "K=bands   # Number of PCA bands\n",
        "x_train = x_train.reshape(-1, windowSize, windowSize, K, 1)\n",
        "x_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(521, 25, 25, 15, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61BWMK9sffa5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e71edf0b-1648-423b-ff9b-21bc059c1790"
      },
      "source": [
        "from keras.utils import np_utils\n",
        "y_train = np_utils.to_categorical(y_train)\n",
        "y_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(521, 13)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cumTl5Tpfk1b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28f3982f-2bd7-481a-986a-b4ba85b11cd5"
      },
      "source": [
        "x_test = x_test.reshape(-1, windowSize, windowSize, K, 1)\n",
        "print(x_test.shape)\n",
        "y_test=np_utils.to_categorical(y_test)\n",
        "print(y_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(4690, 25, 25, 15, 1)\n",
            "(4690, 13)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cK3VjudxkWds",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a92087fc-6ac3-49ad-ef13-33ac6e7d23f3"
      },
      "source": [
        "model=Sequential()\n",
        "conv3D_1=Conv3D(40,(7,7,7),padding='valid',input_shape=(windowSize,windowSize,K,1))\n",
        "conv3D_1r=conv3D_1\n",
        "model.add(conv3D_1r)\n",
        "model.add(Activation('relu'))\n",
        "print(conv3D_1r.output_shape)\n",
        "conv3D_2=Conv3D(40,(3,3,3))\n",
        "conv3D_2r=conv3D_2\n",
        "model.add(conv3D_2r)\n",
        "model.add(Activation('relu'))\n",
        "c=conv3D_2r.output_shape # Getting output shape of the layer\n",
        "print(c)\n",
        "reshape3D_4r = Reshape((c[1], c[2], c[3]*c[4])) # Reshape Layer for 3D to 2D\n",
        "model.add(reshape3D_4r)\n",
        "\n",
        "print(reshape3D_4r.output_shape)\n",
        "#-----Convolution 2D Layer\n",
        "conv2D_1=Conv2D(filters=32, kernel_size=(4,4))\n",
        "conv2D_1r=conv2D_1\n",
        "model.add(conv2D_1r)\n",
        "model.add(Activation('relu'))\n",
        "#odel.add(MaxPooling2D(pool_size=(2,2)))\n",
        "conv2D_2=Conv2D(filters=64, kernel_size=(3,3))\n",
        "conv2D_2r=conv2D_2\n",
        "model.add(conv2D_2r)\n",
        "model.add(Activation('relu'))\n",
        "x=conv2D_2r.output_shape # Getting output shape of the layer\n",
        "print(x)\n",
        "reshape1D= Reshape((x[1],x[2]*x[3]))\n",
        "model.add(reshape1D)\n",
        "print(reshape1D.output_shape)\n",
        "con1D=Conv1D(filters=62, kernel_size=(5))\n",
        "con1D1r=con1D\n",
        "model.add(con1D1r)\n",
        "model.add(Activation('relu'))\n",
        "model.add(Flatten())\n",
        "D1=Dense(units=256)\n",
        "D2=Dense(units=128)\n",
        "D1r=D1\n",
        "D2r=D2\n",
        "model.add(D1r)\n",
        "model.add(Dropout(0.4))\n",
        "model.add(D2r)\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.4))\n",
        "output_units=13\n",
        "model.add(Dense(units=output_units, activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(None, 19, 19, 9, 40)\n",
            "(None, 17, 17, 7, 40)\n",
            "(None, 17, 17, 280)\n",
            "(None, 12, 12, 64)\n",
            "(None, 12, 768)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "myGSI5B4fs6L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40405bba-d179-4180-f18c-c700085b37a4"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv3d_2 (Conv3D)            (None, 19, 19, 9, 40)     13760     \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 19, 19, 9, 40)     0         \n",
            "_________________________________________________________________\n",
            "conv3d_3 (Conv3D)            (None, 17, 17, 7, 40)     43240     \n",
            "_________________________________________________________________\n",
            "activation_7 (Activation)    (None, 17, 17, 7, 40)     0         \n",
            "_________________________________________________________________\n",
            "reshape_2 (Reshape)          (None, 17, 17, 280)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 14, 14, 32)        143392    \n",
            "_________________________________________________________________\n",
            "activation_8 (Activation)    (None, 14, 14, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 12, 12, 64)        18496     \n",
            "_________________________________________________________________\n",
            "activation_9 (Activation)    (None, 12, 12, 64)        0         \n",
            "_________________________________________________________________\n",
            "reshape_3 (Reshape)          (None, 12, 768)           0         \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, 8, 62)             238142    \n",
            "_________________________________________________________________\n",
            "activation_10 (Activation)   (None, 8, 62)             0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 496)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 256)               127232    \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "activation_11 (Activation)   (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 13)                1677      \n",
            "=================================================================\n",
            "Total params: 618,835\n",
            "Trainable params: 618,835\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tjt_30nmmf5H"
      },
      "source": [
        "Adam(lr=0.001, decay=1e-06)\n",
        "model.compile(loss='categorical_crossentropy',optimizer='Adam',metrics=['accuracy'])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nYoILwejjsbx"
      },
      "source": [
        "  logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "  tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
        "    log_dir='./logs', histogram_freq=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBhYQkHDmuJP"
      },
      "source": [
        "filepath = \"bestmodel.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "callbacks_list = [checkpoint]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hDmRADIpmx7I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f8dda98-d4c5-463b-e363-66da5151d1e8"
      },
      "source": [
        "#history=model.fit(x_train, y_train, batch_size=200, epochs=250, validation_split=0.0, verbose=1, callbacks=[tensorboard_callback,checkpoint])\n",
        "history=model.fit(x_train, y_train, batch_size=200, epochs=250, validation_split=0.0, verbose=1, callbacks=callbacks_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/250\n",
            "3/3 [==============================] - 1s 68ms/step - loss: 2.5380 - accuracy: 0.1105\n",
            "\n",
            "Epoch 00001: loss improved from inf to 2.51793, saving model to bestmodel.hdf5\n",
            "Epoch 2/250\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 2.1058 - accuracy: 0.2994\n",
            "\n",
            "Epoch 00002: loss improved from 2.51793 to 2.06197, saving model to bestmodel.hdf5\n",
            "Epoch 3/250\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 1.7428 - accuracy: 0.3814\n",
            "\n",
            "Epoch 00003: loss improved from 2.06197 to 1.72146, saving model to bestmodel.hdf5\n",
            "Epoch 4/250\n",
            "3/3 [==============================] - 0s 91ms/step - loss: 1.5082 - accuracy: 0.4616\n",
            "\n",
            "Epoch 00004: loss improved from 1.72146 to 1.50650, saving model to bestmodel.hdf5\n",
            "Epoch 5/250\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 1.3853 - accuracy: 0.5273\n",
            "\n",
            "Epoch 00005: loss improved from 1.50650 to 1.34697, saving model to bestmodel.hdf5\n",
            "Epoch 6/250\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 1.1578 - accuracy: 0.5773\n",
            "\n",
            "Epoch 00006: loss improved from 1.34697 to 1.13226, saving model to bestmodel.hdf5\n",
            "Epoch 7/250\n",
            "3/3 [==============================] - 0s 90ms/step - loss: 0.9768 - accuracy: 0.6181\n",
            "\n",
            "Epoch 00007: loss improved from 1.13226 to 0.98960, saving model to bestmodel.hdf5\n",
            "Epoch 8/250\n",
            "3/3 [==============================] - 0s 55ms/step - loss: 0.9006 - accuracy: 0.6547\n",
            "\n",
            "Epoch 00008: loss improved from 0.98960 to 0.86510, saving model to bestmodel.hdf5\n",
            "Epoch 9/250\n",
            "3/3 [==============================] - 0s 62ms/step - loss: 0.7709 - accuracy: 0.6981\n",
            "\n",
            "Epoch 00009: loss improved from 0.86510 to 0.78573, saving model to bestmodel.hdf5\n",
            "Epoch 10/250\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 0.7050 - accuracy: 0.7491\n",
            "\n",
            "Epoch 00010: loss improved from 0.78573 to 0.68128, saving model to bestmodel.hdf5\n",
            "Epoch 11/250\n",
            "3/3 [==============================] - 0s 90ms/step - loss: 0.6447 - accuracy: 0.7799\n",
            "\n",
            "Epoch 00011: loss improved from 0.68128 to 0.62647, saving model to bestmodel.hdf5\n",
            "Epoch 12/250\n",
            "3/3 [==============================] - 0s 55ms/step - loss: 0.4868 - accuracy: 0.8549\n",
            "\n",
            "Epoch 00012: loss improved from 0.62647 to 0.47081, saving model to bestmodel.hdf5\n",
            "Epoch 13/250\n",
            "3/3 [==============================] - 0s 89ms/step - loss: 0.4559 - accuracy: 0.8289\n",
            "\n",
            "Epoch 00013: loss did not improve from 0.47081\n",
            "Epoch 14/250\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 0.4519 - accuracy: 0.8513\n",
            "\n",
            "Epoch 00014: loss did not improve from 0.47081\n",
            "Epoch 15/250\n",
            "3/3 [==============================] - 0s 90ms/step - loss: 0.3972 - accuracy: 0.8688\n",
            "\n",
            "Epoch 00015: loss improved from 0.47081 to 0.39766, saving model to bestmodel.hdf5\n",
            "Epoch 16/250\n",
            "3/3 [==============================] - 0s 55ms/step - loss: 0.3548 - accuracy: 0.8752\n",
            "\n",
            "Epoch 00016: loss improved from 0.39766 to 0.35947, saving model to bestmodel.hdf5\n",
            "Epoch 17/250\n",
            "3/3 [==============================] - 0s 89ms/step - loss: 0.3184 - accuracy: 0.8969\n",
            "\n",
            "Epoch 00017: loss improved from 0.35947 to 0.34566, saving model to bestmodel.hdf5\n",
            "Epoch 18/250\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 0.2506 - accuracy: 0.9171\n",
            "\n",
            "Epoch 00018: loss improved from 0.34566 to 0.26880, saving model to bestmodel.hdf5\n",
            "Epoch 19/250\n",
            "3/3 [==============================] - 0s 90ms/step - loss: 0.2222 - accuracy: 0.9340\n",
            "\n",
            "Epoch 00019: loss improved from 0.26880 to 0.21434, saving model to bestmodel.hdf5\n",
            "Epoch 20/250\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 0.2341 - accuracy: 0.9267\n",
            "\n",
            "Epoch 00020: loss improved from 0.21434 to 0.19920, saving model to bestmodel.hdf5\n",
            "Epoch 21/250\n",
            "3/3 [==============================] - 0s 92ms/step - loss: 0.1270 - accuracy: 0.9447\n",
            "\n",
            "Epoch 00021: loss improved from 0.19920 to 0.13134, saving model to bestmodel.hdf5\n",
            "Epoch 22/250\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 0.1564 - accuracy: 0.9545\n",
            "\n",
            "Epoch 00022: loss did not improve from 0.13134\n",
            "Epoch 23/250\n",
            "3/3 [==============================] - 0s 90ms/step - loss: 0.1550 - accuracy: 0.9608\n",
            "\n",
            "Epoch 00023: loss did not improve from 0.13134\n",
            "Epoch 24/250\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 0.0937 - accuracy: 0.9674\n",
            "\n",
            "Epoch 00024: loss improved from 0.13134 to 0.10234, saving model to bestmodel.hdf5\n",
            "Epoch 25/250\n",
            "3/3 [==============================] - 0s 91ms/step - loss: 0.1888 - accuracy: 0.9453\n",
            "\n",
            "Epoch 00025: loss did not improve from 0.10234\n",
            "Epoch 26/250\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 0.0690 - accuracy: 0.9747\n",
            "\n",
            "Epoch 00026: loss improved from 0.10234 to 0.07074, saving model to bestmodel.hdf5\n",
            "Epoch 27/250\n",
            "3/3 [==============================] - 0s 92ms/step - loss: 0.1081 - accuracy: 0.9681\n",
            "\n",
            "Epoch 00027: loss did not improve from 0.07074\n",
            "Epoch 28/250\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 0.0830 - accuracy: 0.9753\n",
            "\n",
            "Epoch 00028: loss did not improve from 0.07074\n",
            "Epoch 29/250\n",
            "3/3 [==============================] - 0s 90ms/step - loss: 0.0865 - accuracy: 0.9712\n",
            "\n",
            "Epoch 00029: loss did not improve from 0.07074\n",
            "Epoch 30/250\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 0.1085 - accuracy: 0.9772\n",
            "\n",
            "Epoch 00030: loss did not improve from 0.07074\n",
            "Epoch 31/250\n",
            "3/3 [==============================] - 0s 89ms/step - loss: 0.0669 - accuracy: 0.9820\n",
            "\n",
            "Epoch 00031: loss improved from 0.07074 to 0.05797, saving model to bestmodel.hdf5\n",
            "Epoch 32/250\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 0.0643 - accuracy: 0.9798\n",
            "\n",
            "Epoch 00032: loss improved from 0.05797 to 0.05165, saving model to bestmodel.hdf5\n",
            "Epoch 33/250\n",
            "3/3 [==============================] - 0s 88ms/step - loss: 0.0401 - accuracy: 0.9855\n",
            "\n",
            "Epoch 00033: loss improved from 0.05165 to 0.03645, saving model to bestmodel.hdf5\n",
            "Epoch 34/250\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 0.0434 - accuracy: 0.9845\n",
            "\n",
            "Epoch 00034: loss did not improve from 0.03645\n",
            "Epoch 35/250\n",
            "3/3 [==============================] - 0s 90ms/step - loss: 0.0247 - accuracy: 0.9927\n",
            "\n",
            "Epoch 00035: loss improved from 0.03645 to 0.02271, saving model to bestmodel.hdf5\n",
            "Epoch 36/250\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 0.0145 - accuracy: 0.9892\n",
            "\n",
            "Epoch 00036: loss improved from 0.02271 to 0.01629, saving model to bestmodel.hdf5\n",
            "Epoch 37/250\n",
            "3/3 [==============================] - 0s 89ms/step - loss: 0.0383 - accuracy: 0.9836\n",
            "\n",
            "Epoch 00037: loss did not improve from 0.01629\n",
            "Epoch 38/250\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 0.0151 - accuracy: 0.9956\n",
            "\n",
            "Epoch 00038: loss improved from 0.01629 to 0.01414, saving model to bestmodel.hdf5\n",
            "Epoch 39/250\n",
            "3/3 [==============================] - 0s 91ms/step - loss: 0.0099 - accuracy: 0.9990\n",
            "\n",
            "Epoch 00039: loss improved from 0.01414 to 0.01183, saving model to bestmodel.hdf5\n",
            "Epoch 40/250\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 0.0249 - accuracy: 0.9886\n",
            "\n",
            "Epoch 00040: loss did not improve from 0.01183\n",
            "Epoch 41/250\n",
            "3/3 [==============================] - 0s 91ms/step - loss: 0.0383 - accuracy: 0.9917\n",
            "\n",
            "Epoch 00041: loss did not improve from 0.01183\n",
            "Epoch 42/250\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 0.0597 - accuracy: 0.9873\n",
            "\n",
            "Epoch 00042: loss did not improve from 0.01183\n",
            "Epoch 43/250\n",
            "3/3 [==============================] - 0s 91ms/step - loss: 0.1085 - accuracy: 0.9820\n",
            "\n",
            "Epoch 00043: loss did not improve from 0.01183\n",
            "Epoch 44/250\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 0.0773 - accuracy: 0.9760\n",
            "\n",
            "Epoch 00044: loss did not improve from 0.01183\n",
            "Epoch 45/250\n",
            "3/3 [==============================] - 0s 90ms/step - loss: 0.0535 - accuracy: 0.9823\n",
            "\n",
            "Epoch 00045: loss did not improve from 0.01183\n",
            "Epoch 46/250\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 0.0901 - accuracy: 0.9766\n",
            "\n",
            "Epoch 00046: loss did not improve from 0.01183\n",
            "Epoch 47/250\n",
            "3/3 [==============================] - 0s 90ms/step - loss: 0.1409 - accuracy: 0.9696\n",
            "\n",
            "Epoch 00047: loss did not improve from 0.01183\n",
            "Epoch 48/250\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 0.1250 - accuracy: 0.9668\n",
            "\n",
            "Epoch 00048: loss did not improve from 0.01183\n",
            "Epoch 49/250\n",
            "3/3 [==============================] - 0s 92ms/step - loss: 0.0991 - accuracy: 0.9706\n",
            "\n",
            "Epoch 00049: loss did not improve from 0.01183\n",
            "Epoch 50/250\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 0.0688 - accuracy: 0.9760\n",
            "\n",
            "Epoch 00050: loss did not improve from 0.01183\n",
            "Epoch 51/250\n",
            "3/3 [==============================] - 0s 91ms/step - loss: 0.0984 - accuracy: 0.9769\n",
            "\n",
            "Epoch 00051: loss did not improve from 0.01183\n",
            "Epoch 52/250\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 0.0812 - accuracy: 0.9801\n",
            "\n",
            "Epoch 00052: loss did not improve from 0.01183\n",
            "Epoch 53/250\n",
            "3/3 [==============================] - 0s 90ms/step - loss: 0.5246 - accuracy: 0.9387\n",
            "\n",
            "Epoch 00053: loss did not improve from 0.01183\n",
            "Epoch 54/250\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 0.2728 - accuracy: 0.9127\n",
            "\n",
            "Epoch 00054: loss did not improve from 0.01183\n",
            "Epoch 55/250\n",
            "3/3 [==============================] - 0s 90ms/step - loss: 0.1617 - accuracy: 0.9367\n",
            "\n",
            "Epoch 00055: loss did not improve from 0.01183\n",
            "Epoch 56/250\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 0.1430 - accuracy: 0.9582\n",
            "\n",
            "Epoch 00056: loss did not improve from 0.01183\n",
            "Epoch 57/250\n",
            "3/3 [==============================] - 0s 91ms/step - loss: 0.1304 - accuracy: 0.9601\n",
            "\n",
            "Epoch 00057: loss did not improve from 0.01183\n",
            "Epoch 58/250\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 0.1260 - accuracy: 0.9511\n",
            "\n",
            "Epoch 00058: loss did not improve from 0.01183\n",
            "Epoch 59/250\n",
            "3/3 [==============================] - 0s 90ms/step - loss: 0.1100 - accuracy: 0.9586\n",
            "\n",
            "Epoch 00059: loss did not improve from 0.01183\n",
            "Epoch 60/250\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 0.0889 - accuracy: 0.9697\n",
            "\n",
            "Epoch 00060: loss did not improve from 0.01183\n",
            "Epoch 61/250\n",
            "3/3 [==============================] - 0s 91ms/step - loss: 0.0385 - accuracy: 0.9899\n",
            "\n",
            "Epoch 00061: loss did not improve from 0.01183\n",
            "Epoch 62/250\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 0.0327 - accuracy: 0.9899\n",
            "\n",
            "Epoch 00062: loss did not improve from 0.01183\n",
            "Epoch 63/250\n",
            "3/3 [==============================] - 0s 63ms/step - loss: 0.0151 - accuracy: 0.9952\n",
            "\n",
            "Epoch 00063: loss did not improve from 0.01183\n",
            "Epoch 64/250\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 0.0140 - accuracy: 0.9956\n",
            "\n",
            "Epoch 00064: loss did not improve from 0.01183\n",
            "Epoch 65/250\n",
            "3/3 [==============================] - 0s 90ms/step - loss: 0.0139 - accuracy: 0.9959\n",
            "\n",
            "Epoch 00065: loss did not improve from 0.01183\n",
            "Epoch 66/250\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 0.0367 - accuracy: 0.9912\n",
            "\n",
            "Epoch 00066: loss did not improve from 0.01183\n",
            "Epoch 67/250\n",
            "3/3 [==============================] - 0s 89ms/step - loss: 0.0127 - accuracy: 0.9972\n",
            "\n",
            "Epoch 00067: loss improved from 0.01183 to 0.01108, saving model to bestmodel.hdf5\n",
            "Epoch 68/250\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 0.0068 - accuracy: 0.9968\n",
            "\n",
            "Epoch 00068: loss improved from 0.01108 to 0.00763, saving model to bestmodel.hdf5\n",
            "Epoch 69/250\n",
            "3/3 [==============================] - 0s 91ms/step - loss: 0.0095 - accuracy: 0.9972\n",
            "\n",
            "Epoch 00069: loss improved from 0.00763 to 0.00715, saving model to bestmodel.hdf5\n",
            "Epoch 70/250\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 0.0052 - accuracy: 0.9984\n",
            "\n",
            "Epoch 00070: loss improved from 0.00715 to 0.00527, saving model to bestmodel.hdf5\n",
            "Epoch 71/250\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 0.0048 - accuracy: 0.9984\n",
            "\n",
            "Epoch 00071: loss improved from 0.00527 to 0.00510, saving model to bestmodel.hdf5\n",
            "Epoch 72/250\n",
            "3/3 [==============================] - 0s 91ms/step - loss: 0.0079 - accuracy: 0.9984\n",
            "\n",
            "Epoch 00072: loss did not improve from 0.00510\n",
            "Epoch 73/250\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 0.0056 - accuracy: 0.9968\n",
            "\n",
            "Epoch 00073: loss did not improve from 0.00510\n",
            "Epoch 74/250\n",
            "3/3 [==============================] - 0s 91ms/step - loss: 0.0020 - accuracy: 1.0000\n",
            "\n",
            "Epoch 00074: loss improved from 0.00510 to 0.00215, saving model to bestmodel.hdf5\n",
            "Epoch 75/250\n",
            "3/3 [==============================] - 0s 56ms/step - loss: 0.0066 - accuracy: 0.9984\n",
            "\n",
            "Epoch 00075: loss did not improve from 0.00215\n",
            "Epoch 76/250\n",
            "3/3 [==============================] - 0s 89ms/step - loss: 6.3082e-04 - accuracy: 1.0000\n",
            "\n",
            "Epoch 00076: loss improved from 0.00215 to 0.00057, saving model to bestmodel.hdf5\n",
            "Epoch 77/250\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 0.0131 - accuracy: 0.9968\n",
            "\n",
            "Epoch 00077: loss did not improve from 0.00057\n",
            "Epoch 78/250\n",
            "3/3 [==============================] - 0s 90ms/step - loss: 0.0200 - accuracy: 0.9955\n",
            "\n",
            "Epoch 00078: loss did not improve from 0.00057\n",
            "Epoch 79/250\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 0.0680 - accuracy: 0.9860\n",
            "\n",
            "Epoch 00079: loss did not improve from 0.00057\n",
            "Epoch 80/250\n",
            "3/3 [==============================] - 0s 90ms/step - loss: 0.0177 - accuracy: 0.9943\n",
            "\n",
            "Epoch 00080: loss did not improve from 0.00057\n",
            "Epoch 81/250\n",
            "3/3 [==============================] - 0s 52ms/step - loss: 0.0202 - accuracy: 0.9871\n",
            "\n",
            "Epoch 00081: loss did not improve from 0.00057\n",
            "Epoch 82/250\n",
            "3/3 [==============================] - 0s 90ms/step - loss: 0.0353 - accuracy: 0.9943\n",
            "\n",
            "Epoch 00082: loss did not improve from 0.00057\n",
            "Epoch 83/250\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 0.0072 - accuracy: 1.0000\n",
            "\n",
            "Epoch 00083: loss did not improve from 0.00057\n",
            "Epoch 84/250\n",
            "3/3 [==============================] - 0s 90ms/step - loss: 0.0178 - accuracy: 0.9937\n",
            "\n",
            "Epoch 00084: loss did not improve from 0.00057\n",
            "Epoch 85/250\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 0.0083 - accuracy: 0.9981\n",
            "\n",
            "Epoch 00085: loss did not improve from 0.00057\n",
            "Epoch 86/250\n",
            "3/3 [==============================] - 0s 90ms/step - loss: 0.0071 - accuracy: 0.9972\n",
            "\n",
            "Epoch 00086: loss did not improve from 0.00057\n",
            "Epoch 87/250\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 0.0114 - accuracy: 0.9962\n",
            "\n",
            "Epoch 00087: loss did not improve from 0.00057\n",
            "Epoch 88/250\n",
            "3/3 [==============================] - 0s 89ms/step - loss: 0.0053 - accuracy: 0.9990\n",
            "\n",
            "Epoch 00088: loss did not improve from 0.00057\n",
            "Epoch 89/250\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 0.0016 - accuracy: 1.0000\n",
            "\n",
            "Epoch 00089: loss did not improve from 0.00057\n",
            "Epoch 90/250\n",
            "3/3 [==============================] - 0s 90ms/step - loss: 0.0058 - accuracy: 0.9972\n",
            "\n",
            "Epoch 00090: loss did not improve from 0.00057\n",
            "Epoch 91/250\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 0.0031 - accuracy: 1.0000\n",
            "\n",
            "Epoch 00091: loss did not improve from 0.00057\n",
            "Epoch 92/250\n",
            "3/3 [==============================] - 0s 91ms/step - loss: 0.0518 - accuracy: 0.9915\n",
            "\n",
            "Epoch 00092: loss did not improve from 0.00057\n",
            "Epoch 93/250\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 0.0056 - accuracy: 0.9984\n",
            "\n",
            "Epoch 00093: loss did not improve from 0.00057\n",
            "Epoch 94/250\n",
            "3/3 [==============================] - 0s 91ms/step - loss: 0.0013 - accuracy: 1.0000\n",
            "\n",
            "Epoch 00094: loss did not improve from 0.00057\n",
            "Epoch 95/250\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 4.2089e-04 - accuracy: 1.0000\n",
            "\n",
            "Epoch 00095: loss improved from 0.00057 to 0.00044, saving model to bestmodel.hdf5\n",
            "Epoch 96/250\n",
            "3/3 [==============================] - 0s 90ms/step - loss: 5.4461e-04 - accuracy: 1.0000\n",
            "\n",
            "Epoch 00096: loss did not improve from 0.00044\n",
            "Epoch 97/250\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 3.1514e-04 - accuracy: 1.0000\n",
            "\n",
            "Epoch 00097: loss improved from 0.00044 to 0.00026, saving model to bestmodel.hdf5\n",
            "Epoch 98/250\n",
            "3/3 [==============================] - 0s 91ms/step - loss: 7.0228e-04 - accuracy: 1.0000\n",
            "\n",
            "Epoch 00098: loss did not improve from 0.00026\n",
            "Epoch 99/250\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 0.0080 - accuracy: 0.9984\n",
            "\n",
            "Epoch 00099: loss did not improve from 0.00026\n",
            "Epoch 100/250\n",
            "3/3 [==============================] - 0s 91ms/step - loss: 0.0010 - accuracy: 1.0000\n",
            "\n",
            "Epoch 00100: loss did not improve from 0.00026\n",
            "Epoch 101/250\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 0.0201 - accuracy: 0.9946\n",
            "\n",
            "Epoch 00101: loss did not improve from 0.00026\n",
            "Epoch 102/250\n",
            "3/3 [==============================] - 0s 91ms/step - loss: 0.0014 - accuracy: 1.0000\n",
            "\n",
            "Epoch 00102: loss did not improve from 0.00026\n",
            "Epoch 103/250\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 0.0042 - accuracy: 0.9984\n",
            "\n",
            "Epoch 00103: loss did not improve from 0.00026\n",
            "Epoch 104/250\n",
            "3/3 [==============================] - 0s 91ms/step - loss: 0.0236 - accuracy: 0.9972\n",
            "\n",
            "Epoch 00104: loss did not improve from 0.00026\n",
            "Epoch 105/250\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 0.0176 - accuracy: 0.9937\n",
            "\n",
            "Epoch 00105: loss did not improve from 0.00026\n",
            "Epoch 106/250\n",
            "3/3 [==============================] - 0s 90ms/step - loss: 0.0069 - accuracy: 0.9984\n",
            "\n",
            "Epoch 00106: loss did not improve from 0.00026\n",
            "Epoch 107/250\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 0.0464 - accuracy: 0.9972\n",
            "\n",
            "Epoch 00107: loss did not improve from 0.00026\n",
            "Epoch 108/250\n",
            "3/3 [==============================] - 0s 93ms/step - loss: 0.0093 - accuracy: 0.9990\n",
            "\n",
            "Epoch 00108: loss did not improve from 0.00026\n",
            "Epoch 109/250\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 0.0191 - accuracy: 0.9971\n",
            "\n",
            "Epoch 00109: loss did not improve from 0.00026\n",
            "Epoch 110/250\n",
            "3/3 [==============================] - 0s 90ms/step - loss: 0.0502 - accuracy: 0.9877\n",
            "\n",
            "Epoch 00110: loss did not improve from 0.00026\n",
            "Epoch 111/250\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 0.0281 - accuracy: 0.9886\n",
            "\n",
            "Epoch 00111: loss did not improve from 0.00026\n",
            "Epoch 112/250\n",
            "3/3 [==============================] - 0s 90ms/step - loss: 0.0113 - accuracy: 0.9952\n",
            "\n",
            "Epoch 00112: loss did not improve from 0.00026\n",
            "Epoch 113/250\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 0.0443 - accuracy: 0.9905\n",
            "\n",
            "Epoch 00113: loss did not improve from 0.00026\n",
            "Epoch 114/250\n",
            "3/3 [==============================] - 0s 93ms/step - loss: 0.0150 - accuracy: 0.9946\n",
            "\n",
            "Epoch 00114: loss did not improve from 0.00026\n",
            "Epoch 115/250\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 0.0556 - accuracy: 0.9968\n",
            "\n",
            "Epoch 00115: loss did not improve from 0.00026\n",
            "Epoch 116/250\n",
            "3/3 [==============================] - 0s 90ms/step - loss: 0.0171 - accuracy: 0.9927\n",
            "\n",
            "Epoch 00116: loss did not improve from 0.00026\n",
            "Epoch 117/250\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 0.0581 - accuracy: 0.9946\n",
            "\n",
            "Epoch 00117: loss did not improve from 0.00026\n",
            "Epoch 118/250\n",
            "3/3 [==============================] - 0s 90ms/step - loss: 0.0159 - accuracy: 0.9952\n",
            "\n",
            "Epoch 00118: loss did not improve from 0.00026\n",
            "Epoch 119/250\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 0.0219 - accuracy: 0.9933\n",
            "\n",
            "Epoch 00119: loss did not improve from 0.00026\n",
            "Epoch 120/250\n",
            "3/3 [==============================] - 0s 89ms/step - loss: 0.0176 - accuracy: 0.9924\n",
            "\n",
            "Epoch 00120: loss did not improve from 0.00026\n",
            "Epoch 121/250\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 0.0038 - accuracy: 1.0000\n",
            "\n",
            "Epoch 00121: loss did not improve from 0.00026\n",
            "Epoch 122/250\n",
            "3/3 [==============================] - 0s 93ms/step - loss: 0.0055 - accuracy: 0.9968\n",
            "\n",
            "Epoch 00122: loss did not improve from 0.00026\n",
            "Epoch 123/250\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 0.0161 - accuracy: 0.9940\n",
            "\n",
            "Epoch 00123: loss did not improve from 0.00026\n",
            "Epoch 124/250\n",
            "3/3 [==============================] - 0s 90ms/step - loss: 0.0037 - accuracy: 1.0000\n",
            "\n",
            "Epoch 00124: loss did not improve from 0.00026\n",
            "Epoch 125/250\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 0.0013 - accuracy: 1.0000\n",
            "\n",
            "Epoch 00125: loss did not improve from 0.00026\n",
            "Epoch 126/250\n",
            "3/3 [==============================] - 0s 89ms/step - loss: 0.0148 - accuracy: 0.9956\n",
            "\n",
            "Epoch 00126: loss did not improve from 0.00026\n",
            "Epoch 127/250\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 0.0059 - accuracy: 0.9962\n",
            "\n",
            "Epoch 00127: loss did not improve from 0.00026\n",
            "Epoch 128/250\n",
            "3/3 [==============================] - 0s 90ms/step - loss: 0.0044 - accuracy: 0.9972\n",
            "\n",
            "Epoch 00128: loss did not improve from 0.00026\n",
            "Epoch 129/250\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 0.0037 - accuracy: 1.0000\n",
            "\n",
            "Epoch 00129: loss did not improve from 0.00026\n",
            "Epoch 130/250\n",
            "3/3 [==============================] - 0s 93ms/step - loss: 0.0036 - accuracy: 1.0000\n",
            "\n",
            "Epoch 00130: loss did not improve from 0.00026\n",
            "Epoch 131/250\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 4.3472e-04 - accuracy: 1.0000\n",
            "\n",
            "Epoch 00131: loss did not improve from 0.00026\n",
            "Epoch 132/250\n",
            "3/3 [==============================] - 0s 91ms/step - loss: 0.0125 - accuracy: 0.9962\n",
            "\n",
            "Epoch 00132: loss did not improve from 0.00026\n",
            "Epoch 133/250\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 0.0105 - accuracy: 0.9990\n",
            "\n",
            "Epoch 00133: loss did not improve from 0.00026\n",
            "Epoch 134/250\n",
            "3/3 [==============================] - 0s 90ms/step - loss: 0.0110 - accuracy: 0.9972\n",
            "\n",
            "Epoch 00134: loss did not improve from 0.00026\n",
            "Epoch 135/250\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 0.0135 - accuracy: 0.9946\n",
            "\n",
            "Epoch 00135: loss did not improve from 0.00026\n",
            "Epoch 136/250\n",
            "3/3 [==============================] - 0s 90ms/step - loss: 0.0013 - accuracy: 1.0000\n",
            "\n",
            "Epoch 00136: loss did not improve from 0.00026\n",
            "Epoch 137/250\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 0.0017 - accuracy: 1.0000\n",
            "\n",
            "Epoch 00137: loss did not improve from 0.00026\n",
            "Epoch 138/250\n",
            "3/3 [==============================] - 0s 92ms/step - loss: 8.9985e-04 - accuracy: 1.0000\n",
            "\n",
            "Epoch 00138: loss did not improve from 0.00026\n",
            "Epoch 139/250\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 0.0027 - accuracy: 1.0000\n",
            "\n",
            "Epoch 00139: loss did not improve from 0.00026\n",
            "Epoch 140/250\n",
            "3/3 [==============================] - 0s 91ms/step - loss: 5.5218e-04 - accuracy: 1.0000\n",
            "\n",
            "Epoch 00140: loss did not improve from 0.00026\n",
            "Epoch 141/250\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 0.0259 - accuracy: 0.9972\n",
            "\n",
            "Epoch 00141: loss did not improve from 0.00026\n",
            "Epoch 142/250\n",
            "3/3 [==============================] - 0s 90ms/step - loss: 3.0440e-04 - accuracy: 1.0000\n",
            "\n",
            "Epoch 00142: loss did not improve from 0.00026\n",
            "Epoch 143/250\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 5.6969e-04 - accuracy: 1.0000\n",
            "\n",
            "Epoch 00143: loss did not improve from 0.00026\n",
            "Epoch 144/250\n",
            "3/3 [==============================] - 0s 89ms/step - loss: 0.0132 - accuracy: 0.9924\n",
            "\n",
            "Epoch 00144: loss did not improve from 0.00026\n",
            "Epoch 145/250\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 4.3883e-04 - accuracy: 1.0000\n",
            "\n",
            "Epoch 00145: loss did not improve from 0.00026\n",
            "Epoch 146/250\n",
            "3/3 [==============================] - 0s 91ms/step - loss: 0.0017 - accuracy: 1.0000\n",
            "\n",
            "Epoch 00146: loss did not improve from 0.00026\n",
            "Epoch 147/250\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 0.0015 - accuracy: 1.0000\n",
            "\n",
            "Epoch 00147: loss did not improve from 0.00026\n",
            "Epoch 148/250\n",
            "3/3 [==============================] - 0s 92ms/step - loss: 4.4968e-04 - accuracy: 1.0000\n",
            "\n",
            "Epoch 00148: loss did not improve from 0.00026\n",
            "Epoch 149/250\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 0.0077 - accuracy: 0.9984\n",
            "\n",
            "Epoch 00149: loss did not improve from 0.00026\n",
            "Epoch 150/250\n",
            "3/3 [==============================] - 0s 90ms/step - loss: 0.0318 - accuracy: 0.9946\n",
            "\n",
            "Epoch 00150: loss did not improve from 0.00026\n",
            "Epoch 151/250\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 0.2948 - accuracy: 0.9930\n",
            "\n",
            "Epoch 00151: loss did not improve from 0.00026\n",
            "Epoch 152/250\n",
            "3/3 [==============================] - 0s 91ms/step - loss: 0.0516 - accuracy: 0.9927\n",
            "\n",
            "Epoch 00152: loss did not improve from 0.00026\n",
            "Epoch 153/250\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 0.3274 - accuracy: 0.9534\n",
            "\n",
            "Epoch 00153: loss did not improve from 0.00026\n",
            "Epoch 154/250\n",
            "3/3 [==============================] - 0s 91ms/step - loss: 0.2942 - accuracy: 0.9614\n",
            "\n",
            "Epoch 00154: loss did not improve from 0.00026\n",
            "Epoch 155/250\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 0.9779 - accuracy: 0.8638\n",
            "\n",
            "Epoch 00155: loss did not improve from 0.00026\n",
            "Epoch 156/250\n",
            "3/3 [==============================] - 0s 90ms/step - loss: 0.7191 - accuracy: 0.9067\n",
            "\n",
            "Epoch 00156: loss did not improve from 0.00026\n",
            "Epoch 157/250\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 0.5210 - accuracy: 0.8892\n",
            "\n",
            "Epoch 00157: loss did not improve from 0.00026\n",
            "Epoch 158/250\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 0.2799 - accuracy: 0.9283\n",
            "\n",
            "Epoch 00158: loss did not improve from 0.00026\n",
            "Epoch 159/250\n",
            "3/3 [==============================] - 0s 90ms/step - loss: 0.5053 - accuracy: 0.9030\n",
            "\n",
            "Epoch 00159: loss did not improve from 0.00026\n",
            "Epoch 160/250\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 0.4385 - accuracy: 0.9006\n",
            "\n",
            "Epoch 00160: loss did not improve from 0.00026\n",
            "Epoch 161/250\n",
            "3/3 [==============================] - 0s 91ms/step - loss: 0.1418 - accuracy: 0.9630\n",
            "\n",
            "Epoch 00161: loss did not improve from 0.00026\n",
            "Epoch 162/250\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 0.0492 - accuracy: 0.9930\n",
            "\n",
            "Epoch 00162: loss did not improve from 0.00026\n",
            "Epoch 163/250\n",
            "3/3 [==============================] - 0s 63ms/step - loss: 0.0410 - accuracy: 0.9902\n",
            "\n",
            "Epoch 00163: loss did not improve from 0.00026\n",
            "Epoch 164/250\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 0.0261 - accuracy: 0.9914\n",
            "\n",
            "Epoch 00164: loss did not improve from 0.00026\n",
            "Epoch 165/250\n",
            "3/3 [==============================] - 0s 92ms/step - loss: 0.0346 - accuracy: 0.9861\n",
            "\n",
            "Epoch 00165: loss did not improve from 0.00026\n",
            "Epoch 166/250\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 0.1349 - accuracy: 0.9804\n",
            "\n",
            "Epoch 00166: loss did not improve from 0.00026\n",
            "Epoch 167/250\n",
            "3/3 [==============================] - 0s 94ms/step - loss: 0.0923 - accuracy: 0.9719\n",
            "\n",
            "Epoch 00167: loss did not improve from 0.00026\n",
            "Epoch 168/250\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 0.0647 - accuracy: 0.9772\n",
            "\n",
            "Epoch 00168: loss did not improve from 0.00026\n",
            "Epoch 169/250\n",
            "3/3 [==============================] - 0s 92ms/step - loss: 0.0413 - accuracy: 0.9870\n",
            "\n",
            "Epoch 00169: loss did not improve from 0.00026\n",
            "Epoch 170/250\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 0.0972 - accuracy: 0.9721\n",
            "\n",
            "Epoch 00170: loss did not improve from 0.00026\n",
            "Epoch 171/250\n",
            "3/3 [==============================] - 0s 93ms/step - loss: 0.0489 - accuracy: 0.9842\n",
            "\n",
            "Epoch 00171: loss did not improve from 0.00026\n",
            "Epoch 172/250\n",
            "3/3 [==============================] - 0s 90ms/step - loss: 0.0670 - accuracy: 0.9819\n",
            "\n",
            "Epoch 00172: loss did not improve from 0.00026\n",
            "Epoch 173/250\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 0.0262 - accuracy: 0.9921\n",
            "\n",
            "Epoch 00173: loss did not improve from 0.00026\n",
            "Epoch 174/250\n",
            "3/3 [==============================] - 0s 89ms/step - loss: 0.0174 - accuracy: 0.9952\n",
            "\n",
            "Epoch 00174: loss did not improve from 0.00026\n",
            "Epoch 175/250\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 0.0072 - accuracy: 0.9984\n",
            "\n",
            "Epoch 00175: loss did not improve from 0.00026\n",
            "Epoch 176/250\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 0.0708 - accuracy: 0.9927\n",
            "\n",
            "Epoch 00176: loss did not improve from 0.00026\n",
            "Epoch 177/250\n",
            "3/3 [==============================] - 0s 92ms/step - loss: 0.0053 - accuracy: 1.0000\n",
            "\n",
            "Epoch 00177: loss did not improve from 0.00026\n",
            "Epoch 178/250\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 0.0167 - accuracy: 0.9934\n",
            "\n",
            "Epoch 00178: loss did not improve from 0.00026\n",
            "Epoch 179/250\n",
            "3/3 [==============================] - 0s 93ms/step - loss: 0.0311 - accuracy: 0.9880\n",
            "\n",
            "Epoch 00179: loss did not improve from 0.00026\n",
            "Epoch 180/250\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 0.0156 - accuracy: 0.9972\n",
            "\n",
            "Epoch 00180: loss did not improve from 0.00026\n",
            "Epoch 181/250\n",
            "3/3 [==============================] - 0s 91ms/step - loss: 0.0577 - accuracy: 0.9842\n",
            "\n",
            "Epoch 00181: loss did not improve from 0.00026\n",
            "Epoch 182/250\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 0.1287 - accuracy: 0.9813\n",
            "\n",
            "Epoch 00182: loss did not improve from 0.00026\n",
            "Epoch 183/250\n",
            "3/3 [==============================] - 0s 90ms/step - loss: 0.3691 - accuracy: 0.9483\n",
            "\n",
            "Epoch 00183: loss did not improve from 0.00026\n",
            "Epoch 184/250\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 0.1788 - accuracy: 0.9571\n",
            "\n",
            "Epoch 00184: loss did not improve from 0.00026\n",
            "Epoch 185/250\n",
            "3/3 [==============================] - 0s 91ms/step - loss: 0.0420 - accuracy: 0.9852\n",
            "\n",
            "Epoch 00185: loss did not improve from 0.00026\n",
            "Epoch 186/250\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 0.0503 - accuracy: 0.9832\n",
            "\n",
            "Epoch 00186: loss did not improve from 0.00026\n",
            "Epoch 187/250\n",
            "3/3 [==============================] - 0s 62ms/step - loss: 0.0115 - accuracy: 0.9918\n",
            "\n",
            "Epoch 00187: loss did not improve from 0.00026\n",
            "Epoch 188/250\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 0.0714 - accuracy: 0.9851\n",
            "\n",
            "Epoch 00188: loss did not improve from 0.00026\n",
            "Epoch 189/250\n",
            "3/3 [==============================] - 0s 91ms/step - loss: 0.0218 - accuracy: 0.9975\n",
            "\n",
            "Epoch 00189: loss did not improve from 0.00026\n",
            "Epoch 190/250\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 0.2896 - accuracy: 0.9393\n",
            "\n",
            "Epoch 00190: loss did not improve from 0.00026\n",
            "Epoch 191/250\n",
            "3/3 [==============================] - 0s 90ms/step - loss: 0.0535 - accuracy: 0.9879\n",
            "\n",
            "Epoch 00191: loss did not improve from 0.00026\n",
            "Epoch 192/250\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 0.0649 - accuracy: 0.9864\n",
            "\n",
            "Epoch 00192: loss did not improve from 0.00026\n",
            "Epoch 193/250\n",
            "3/3 [==============================] - 0s 90ms/step - loss: 0.0225 - accuracy: 0.9930\n",
            "\n",
            "Epoch 00193: loss did not improve from 0.00026\n",
            "Epoch 194/250\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 0.0491 - accuracy: 0.9801\n",
            "\n",
            "Epoch 00194: loss did not improve from 0.00026\n",
            "Epoch 195/250\n",
            "3/3 [==============================] - 0s 90ms/step - loss: 0.0218 - accuracy: 0.9943\n",
            "\n",
            "Epoch 00195: loss did not improve from 0.00026\n",
            "Epoch 196/250\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 0.0657 - accuracy: 0.9930\n",
            "\n",
            "Epoch 00196: loss did not improve from 0.00026\n",
            "Epoch 197/250\n",
            "3/3 [==============================] - 0s 90ms/step - loss: 0.0166 - accuracy: 0.9927\n",
            "\n",
            "Epoch 00197: loss did not improve from 0.00026\n",
            "Epoch 198/250\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 0.0067 - accuracy: 0.9972\n",
            "\n",
            "Epoch 00198: loss did not improve from 0.00026\n",
            "Epoch 199/250\n",
            "3/3 [==============================] - 0s 93ms/step - loss: 0.0056 - accuracy: 0.9972\n",
            "\n",
            "Epoch 00199: loss did not improve from 0.00026\n",
            "Epoch 200/250\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 0.0307 - accuracy: 0.9927\n",
            "\n",
            "Epoch 00200: loss did not improve from 0.00026\n",
            "Epoch 201/250\n",
            "3/3 [==============================] - 0s 92ms/step - loss: 0.0039 - accuracy: 0.9984\n",
            "\n",
            "Epoch 00201: loss did not improve from 0.00026\n",
            "Epoch 202/250\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 0.1603 - accuracy: 0.9975\n",
            "\n",
            "Epoch 00202: loss did not improve from 0.00026\n",
            "Epoch 203/250\n",
            "3/3 [==============================] - 0s 90ms/step - loss: 0.0012 - accuracy: 1.0000\n",
            "\n",
            "Epoch 00203: loss did not improve from 0.00026\n",
            "Epoch 204/250\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 0.0172 - accuracy: 0.9965\n",
            "\n",
            "Epoch 00204: loss did not improve from 0.00026\n",
            "Epoch 205/250\n",
            "3/3 [==============================] - 0s 91ms/step - loss: 0.1239 - accuracy: 0.9927\n",
            "\n",
            "Epoch 00205: loss did not improve from 0.00026\n",
            "Epoch 206/250\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 5.9622e-04 - accuracy: 1.0000\n",
            "\n",
            "Epoch 00206: loss did not improve from 0.00026\n",
            "Epoch 207/250\n",
            "3/3 [==============================] - 0s 90ms/step - loss: 0.0011 - accuracy: 1.0000\n",
            "\n",
            "Epoch 00207: loss did not improve from 0.00026\n",
            "Epoch 208/250\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 0.0215 - accuracy: 0.9946\n",
            "\n",
            "Epoch 00208: loss did not improve from 0.00026\n",
            "Epoch 209/250\n",
            "3/3 [==============================] - 0s 90ms/step - loss: 0.0083 - accuracy: 0.9984\n",
            "\n",
            "Epoch 00209: loss did not improve from 0.00026\n",
            "Epoch 210/250\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 0.0058 - accuracy: 0.9962\n",
            "\n",
            "Epoch 00210: loss did not improve from 0.00026\n",
            "Epoch 211/250\n",
            "3/3 [==============================] - 0s 90ms/step - loss: 0.0020 - accuracy: 1.0000\n",
            "\n",
            "Epoch 00211: loss did not improve from 0.00026\n",
            "Epoch 212/250\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 0.0597 - accuracy: 0.9940\n",
            "\n",
            "Epoch 00212: loss did not improve from 0.00026\n",
            "Epoch 213/250\n",
            "3/3 [==============================] - 0s 90ms/step - loss: 0.0130 - accuracy: 0.9968\n",
            "\n",
            "Epoch 00213: loss did not improve from 0.00026\n",
            "Epoch 214/250\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 5.1832e-04 - accuracy: 1.0000\n",
            "\n",
            "Epoch 00214: loss did not improve from 0.00026\n",
            "Epoch 215/250\n",
            "3/3 [==============================] - 0s 90ms/step - loss: 8.3418e-04 - accuracy: 1.0000\n",
            "\n",
            "Epoch 00215: loss did not improve from 0.00026\n",
            "Epoch 216/250\n",
            "3/3 [==============================] - 0s 52ms/step - loss: 0.0040 - accuracy: 0.9972\n",
            "\n",
            "Epoch 00216: loss did not improve from 0.00026\n",
            "Epoch 217/250\n",
            "3/3 [==============================] - 0s 90ms/step - loss: 1.0212e-04 - accuracy: 1.0000\n",
            "\n",
            "Epoch 00217: loss improved from 0.00026 to 0.00009, saving model to bestmodel.hdf5\n",
            "Epoch 218/250\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 0.0358 - accuracy: 0.9962\n",
            "\n",
            "Epoch 00218: loss did not improve from 0.00009\n",
            "Epoch 219/250\n",
            "3/3 [==============================] - 0s 91ms/step - loss: 1.3079e-04 - accuracy: 1.0000\n",
            "\n",
            "Epoch 00219: loss did not improve from 0.00009\n",
            "Epoch 220/250\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 0.0012 - accuracy: 1.0000\n",
            "\n",
            "Epoch 00220: loss did not improve from 0.00009\n",
            "Epoch 221/250\n",
            "3/3 [==============================] - 0s 89ms/step - loss: 0.0035 - accuracy: 0.9990\n",
            "\n",
            "Epoch 00221: loss did not improve from 0.00009\n",
            "Epoch 222/250\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 0.0160 - accuracy: 0.9956\n",
            "\n",
            "Epoch 00222: loss did not improve from 0.00009\n",
            "Epoch 223/250\n",
            "3/3 [==============================] - 0s 91ms/step - loss: 0.0039 - accuracy: 0.9990\n",
            "\n",
            "Epoch 00223: loss did not improve from 0.00009\n",
            "Epoch 224/250\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 0.0041 - accuracy: 0.9972\n",
            "\n",
            "Epoch 00224: loss did not improve from 0.00009\n",
            "Epoch 225/250\n",
            "3/3 [==============================] - 0s 91ms/step - loss: 0.0096 - accuracy: 0.9972\n",
            "\n",
            "Epoch 00225: loss did not improve from 0.00009\n",
            "Epoch 226/250\n",
            "3/3 [==============================] - 0s 61ms/step - loss: 0.0380 - accuracy: 0.9984\n",
            "\n",
            "Epoch 00226: loss did not improve from 0.00009\n",
            "Epoch 227/250\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 0.0011 - accuracy: 1.0000\n",
            "\n",
            "Epoch 00227: loss did not improve from 0.00009\n",
            "Epoch 228/250\n",
            "3/3 [==============================] - 0s 91ms/step - loss: 0.0067 - accuracy: 0.9990\n",
            "\n",
            "Epoch 00228: loss did not improve from 0.00009\n",
            "Epoch 229/250\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 0.0103 - accuracy: 0.9956\n",
            "\n",
            "Epoch 00229: loss did not improve from 0.00009\n",
            "Epoch 230/250\n",
            "3/3 [==============================] - 0s 90ms/step - loss: 6.2052e-04 - accuracy: 1.0000\n",
            "\n",
            "Epoch 00230: loss did not improve from 0.00009\n",
            "Epoch 231/250\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 0.0172 - accuracy: 0.9962\n",
            "\n",
            "Epoch 00231: loss did not improve from 0.00009\n",
            "Epoch 232/250\n",
            "3/3 [==============================] - 0s 90ms/step - loss: 0.0069 - accuracy: 0.9984\n",
            "\n",
            "Epoch 00232: loss did not improve from 0.00009\n",
            "Epoch 233/250\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 0.0209 - accuracy: 0.9975\n",
            "\n",
            "Epoch 00233: loss did not improve from 0.00009\n",
            "Epoch 234/250\n",
            "3/3 [==============================] - 0s 91ms/step - loss: 0.0216 - accuracy: 0.9959\n",
            "\n",
            "Epoch 00234: loss did not improve from 0.00009\n",
            "Epoch 235/250\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 6.6997e-04 - accuracy: 1.0000\n",
            "\n",
            "Epoch 00235: loss did not improve from 0.00009\n",
            "Epoch 236/250\n",
            "3/3 [==============================] - 0s 63ms/step - loss: 0.0143 - accuracy: 0.9930\n",
            "\n",
            "Epoch 00236: loss did not improve from 0.00009\n",
            "Epoch 237/250\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 0.0017 - accuracy: 1.0000\n",
            "\n",
            "Epoch 00237: loss did not improve from 0.00009\n",
            "Epoch 238/250\n",
            "3/3 [==============================] - 0s 90ms/step - loss: 0.0016 - accuracy: 0.9984\n",
            "\n",
            "Epoch 00238: loss did not improve from 0.00009\n",
            "Epoch 239/250\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 0.0102 - accuracy: 0.9984\n",
            "\n",
            "Epoch 00239: loss did not improve from 0.00009\n",
            "Epoch 240/250\n",
            "3/3 [==============================] - 0s 90ms/step - loss: 7.8767e-04 - accuracy: 1.0000\n",
            "\n",
            "Epoch 00240: loss did not improve from 0.00009\n",
            "Epoch 241/250\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 7.0425e-04 - accuracy: 1.0000\n",
            "\n",
            "Epoch 00241: loss did not improve from 0.00009\n",
            "Epoch 242/250\n",
            "3/3 [==============================] - 0s 93ms/step - loss: 0.0017 - accuracy: 1.0000\n",
            "\n",
            "Epoch 00242: loss did not improve from 0.00009\n",
            "Epoch 243/250\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 0.0035 - accuracy: 0.9990\n",
            "\n",
            "Epoch 00243: loss did not improve from 0.00009\n",
            "Epoch 244/250\n",
            "3/3 [==============================] - 0s 90ms/step - loss: 1.8243e-04 - accuracy: 1.0000\n",
            "\n",
            "Epoch 00244: loss did not improve from 0.00009\n",
            "Epoch 245/250\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 1.3010e-04 - accuracy: 1.0000\n",
            "\n",
            "Epoch 00245: loss did not improve from 0.00009\n",
            "Epoch 246/250\n",
            "3/3 [==============================] - 0s 91ms/step - loss: 4.2350e-04 - accuracy: 1.0000\n",
            "\n",
            "Epoch 00246: loss did not improve from 0.00009\n",
            "Epoch 247/250\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 0.0025 - accuracy: 0.9984\n",
            "\n",
            "Epoch 00247: loss did not improve from 0.00009\n",
            "Epoch 248/250\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 1.4205e-04 - accuracy: 1.0000\n",
            "\n",
            "Epoch 00248: loss did not improve from 0.00009\n",
            "Epoch 249/250\n",
            "3/3 [==============================] - 0s 90ms/step - loss: 0.0022 - accuracy: 0.9972\n",
            "\n",
            "Epoch 00249: loss did not improve from 0.00009\n",
            "Epoch 250/250\n",
            "3/3 [==============================] - 0s 127ms/step - loss: 4.4661e-04 - accuracy: 1.0000\n",
            "\n",
            "Epoch 00250: loss did not improve from 0.00009\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B6kf-kzrm2wb"
      },
      "source": [
        "model.load_weights(\"bestmodel.hdf5\")               # loading weights from trained model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PgZ8o4nc-nOZ"
      },
      "source": [
        "#%tensorboard --logdir logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2Oorny4m_Ab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8fc6685-025b-42cc-ee90-8557587ea36b"
      },
      "source": [
        "y_pred= model.predict(x_test)\n",
        "y_pred= np.argmax(y_pred, axis=1)\n",
        "classification = classification_report(np.argmax(y_test, axis=1), y_pred)\n",
        "print(classification)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.99      0.99       685\n",
            "           1       1.00      0.97      0.98       219\n",
            "           2       0.92      1.00      0.96       230\n",
            "           3       1.00      0.91      0.95       227\n",
            "           4       0.94      0.94      0.94       145\n",
            "           5       0.98      0.98      0.98       206\n",
            "           6       0.97      1.00      0.98        94\n",
            "           7       1.00      1.00      1.00       388\n",
            "           8       1.00      1.00      1.00       468\n",
            "           9       1.00      0.99      0.99       364\n",
            "          10       1.00      1.00      1.00       377\n",
            "          11       0.98      1.00      0.99       453\n",
            "          12       1.00      1.00      1.00       834\n",
            "\n",
            "    accuracy                           0.99      4690\n",
            "   macro avg       0.98      0.98      0.98      4690\n",
            "weighted avg       0.99      0.99      0.99      4690\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XA9z1ht-nDYy"
      },
      "source": [
        "def AA_andEachClassAccuracy(confusion_matrix):\n",
        "    counter = confusion_matrix.shape[0]\n",
        "    list_diag = np.diag(confusion_matrix)\n",
        "    list_raw_sum = np.sum(confusion_matrix, axis=1)\n",
        "    each_acc = np.nan_to_num(truediv(list_diag, list_raw_sum))\n",
        "    average_acc = np.mean(each_acc)\n",
        "    return each_acc, average_acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGUxgmLAnI_T",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 713
        },
        "outputId": "e4e4208d-4bb9-4bed-e725-b20a30c7dc11"
      },
      "source": [
        "oa = accuracy_score(np.argmax(y_test, axis=1), y_pred)\n",
        "confusion = confusion_matrix(np.argmax(y_test, axis=1), y_pred)\n",
        "each_acc, aa = AA_andEachClassAccuracy(confusion)\n",
        "kappa = cohen_kappa_score(np.argmax(y_test, axis=1), y_pred)\n",
        "score = model.evaluate(x_test, y_test, batch_size=36)\n",
        "Test_Loss =  score[0]*100\n",
        "Test_accuracy = score[1]*100\n",
        "print(oa)\n",
        "print(kappa)\n",
        "print(aa)\n",
        "#print (confusion)\n",
        "a=confusion.diagonal()/confusion.sum(axis=0)\n",
        "print(a)\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "sn.heatmap(confusion, annot=True, cmap=plt.cm.Reds,fmt=\"d\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "131/131 [==============================] - 1s 8ms/step - loss: 0.0755 - accuracy: 0.9891\n",
            "0.9897654584221749\n",
            "0.9886054744742787\n",
            "0.9835204025763233\n",
            "[0.99561404 0.99530516 0.92       0.99519231 0.94482759 0.98058252\n",
            " 0.96907216 1.         1.         1.         1.         0.98264642\n",
            " 1.        ]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rA96DDpU3UE7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad472dc0-46c5-4bf0-8a41-627a4e469f71"
      },
      "source": [
        "print(x_test[1].shape)\n",
        "a=x_test[5].reshape(-1,25,25,15,1)\n",
        "otest=x_test[5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(25, 25, 15, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-e6V53qo3Usb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        },
        "outputId": "f965474e-ef99-4260-c987-2ee4eea3787b"
      },
      "source": [
        "featuremap=actmodel.predict(a)\n",
        "print (featuremap.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-46-41b5f032fe83>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfeaturemap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mactmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfeaturemap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'actmodel' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ZWy-Pos3l7M",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "outputId": "0c06472e-cabc-4528-b1d5-d1d5ea42ad9a"
      },
      "source": [
        "print ('Original Image')\n",
        "spectral.imshow(otest[:,:,:,0],(8,10,10),figsize=(1.5,1.5))\n",
        "c=featuremap.reshape(25,25,15,1)\n",
        "print ('Itermediate Model Image')\n",
        "for i in range(32):\n",
        "  select=c[:,:,:,i]\n",
        "  spectral.imshow(select,(8,10,10),figsize=(1.5,1.5),fignum=i)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original Image\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-48-aca9b0b70553>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'Original Image'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mspectral\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0motest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeaturemap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'Itermediate Model Image'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'featuremap' is not defined"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 108x108 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JgOz3ALh3pw0"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ko5L25-xktgY"
      },
      "source": [
        "def Patch(data,height_index,width_index):\n",
        "    height_slice = slice(height_index, height_index+PATCH_SIZE)\n",
        "    width_slice = slice(width_index, width_index+PATCH_SIZE)\n",
        "    patch = data[height_slice, width_slice, :]\n",
        "    return patch\n",
        "\n",
        "height = groundtruth.shape[0]\n",
        "width = groundtruth.shape[1]\n",
        "PATCH_SIZE = 25\n",
        "numComponents = K\n",
        "X = padWithZeros(PCAbandFinalarray, PATCH_SIZE//2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHtFdcLrkt4T"
      },
      "source": [
        "# calculate the predicted image\n",
        "outputsRelu= np.zeros((height,width))\n",
        "for i in range(height):\n",
        "    for j in range(width):\n",
        "        target = int(groundtruth[i,j])\n",
        "        if target == 0 :\n",
        "            continue\n",
        "        else :\n",
        "            image_patch=Patch(X,i,j)\n",
        "            X_test_image = image_patch.reshape(1,image_patch.shape[0],image_patch.shape[1], image_patch.shape[2], 1).astype('float32')\n",
        "            predictionrelu = (model.predict(X_test_image))\n",
        "            predictionrelu = np.argmax(predictionrelu, axis=1)\n",
        "            outputsRelu[i][j] = predictionrelu+1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1HXDbHHkuKL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 517
        },
        "outputId": "a3554e6b-2187-48e6-f73d-f3a641cde5a0"
      },
      "source": [
        "predict_image = spectral.imshow(classes = outputsRelu.astype(int),figsize =(10,10))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "id": "weMRSubz4egc",
        "outputId": "c52b3d3b-f6ba-4ec1-d2c1-ea4d2034689c"
      },
      "source": [
        "ground_truth = spectral.imshow(classes = groundtruth,figsize =(7,7))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 504x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7VecizyfnPBH"
      },
      "source": [
        "filepath1 = \"bestmodel_swish.hdf5\"\n",
        "checkpoint_swish = ModelCheckpoint(filepath1, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "callbacks_list_s= [checkpoint_swish]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fz1H4pzZIKya"
      },
      "source": [
        "  logdir = os.path.join(\"logs1\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "  tensorboard_callback1 = tf.keras.callbacks.TensorBoard(log_dir='./logs1', histogram_freq=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RbxwjYgJkirU"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8nr2j6ECnr1u"
      },
      "source": [
        "print(history.history.keys())\n",
        "#  \"Accuracy\"\n",
        "plt.figure(figsize=(10,8))\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history_swish.history['accuracy'])\n",
        "\n",
        "#plt.plot(history.history['val_acc'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['Relu', 'Swish'], loc='upper left')\n",
        "plt.show()\n",
        "# \"Loss\"\n",
        "plt.figure(figsize=(10,8))\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history_swish.history['loss'])\n",
        "\n",
        "#plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['Relu', 'Swish'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3i5amadE3Gh3"
      },
      "source": [
        "def Patch(data,height_index,width_index):\n",
        "    height_slice = slice(height_index, height_index+PATCH_SIZE)\n",
        "    width_slice = slice(width_index, width_index+PATCH_SIZE)\n",
        "    patch = data[height_slice, width_slice, :]\n",
        "\n",
        "    return patch\n",
        "\n",
        "height = groundtruth.shape[0]\n",
        "width = groundtruth.shape[1]\n",
        "PATCH_SIZE = 27\n",
        "numComponents = K\n",
        "X = padWithZeros(PCAbandFinalarray, PATCH_SIZE//2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8RiMOuuEBtQ1"
      },
      "source": [
        "outputsSwish= np.zeros((height,width))\n",
        "for i in range(height):\n",
        "    for j in range(width):\n",
        "        target = int(groundtruth[i,j])\n",
        "        if target == 0 :\n",
        "            continue\n",
        "        else :\n",
        "            image_patch=Patch(X,i,j)\n",
        "            X_test_image = image_patch.reshape(1,image_patch.shape[0],image_patch.shape[1], image_patch.shape[2], 1).astype('float32')\n",
        "            predictionswish = (model_swish.predict(X_test_image))\n",
        "            predictionswish = np.argmax(predictionswish, axis=1)\n",
        "            outputsSwish[i][j] = predictionswish+1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_wfnORN63G5g"
      },
      "source": [
        "predict_image = spectral.imshow(classes = outputsSwish.astype(int),figsize =(10,10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GUBFtbpD3G8u"
      },
      "source": [
        "predict_image = spectral.imshow(classes =groundtruth,figsize =(10,10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5FyUCiP4PjIS"
      },
      "source": [
        "layer_outputs = [layer.output for layer in model_swish.layers ]\n",
        "print (layer_outputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pw2fjDOIPT6t"
      },
      "source": [
        "actmodel1 = tf.keras.models.Model(inputs=model_swish.input,outputs=model_swish.get_layer('activation_1').output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCtuvsLSPltW"
      },
      "source": [
        "b=x_test[5].reshape(-1,27,27,20,1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HGSz66lJPxzc"
      },
      "source": [
        "featuremap1=actmodel1.predict(b)\n",
        "print (featuremap1.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXHEXLBHP8Ww"
      },
      "source": [
        "print ('Original Image')\n",
        "spectral.imshow(otest[:,:,:,0],(8,10,10))\n",
        "b=featuremap1.reshape(21,21,16,32)\n",
        "for i in range(32):\n",
        "  select=b[:,:,:,i]\n",
        "  spectral.imshow(select,(8,10,10),figsize=(1.5,1.5),fignum=i)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}